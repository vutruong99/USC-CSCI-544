{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b4bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1159e2",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation\n",
    "We will use the Amazon reviews dataset used in HW1. Load the dataset and build a balanced dataset of 60K reviews along with their ratings to create labels through random selection similar to HW1. You can store your dataset after generation and reuse it to reduce the computational load. For your experiments consider a 80%/20% training/testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0f858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data while skipping bad lines.\n",
    "\n",
    "dataframe = pd.read_table(\"amazon_reviews_us_Beauty_v1_00.tsv\", error_bad_lines = False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f55136",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_and_ratings = dataframe[[\"review_body\", \"star_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b46a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to numerical values (\"1.0\" -> 1.0) and mark as None if the value is invalid.\n",
    "\n",
    "reviews_and_ratings.loc[: , \"star_rating\"] = pd.to_numeric(reviews_and_ratings[\"star_rating\"], errors = 'coerce', downcast = 'integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b5b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values.\n",
    "\n",
    "reviews_and_ratings = reviews_and_ratings.dropna(how = \"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d49da5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094302</th>\n",
       "      <td>After watching my Dad struggle with his scisso...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094303</th>\n",
       "      <td>Like most sound machines, the sounds choices a...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094304</th>\n",
       "      <td>I bought this product because it indicated 30 ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094305</th>\n",
       "      <td>We have used Oral-B products for 15 years; thi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094306</th>\n",
       "      <td>I love this toothbrush. It's easy to use, and ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093907 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating\n",
       "0                         Love this, excellent sun block!!          5.0\n",
       "1        The great thing about this cream is that it do...          5.0\n",
       "2        Great Product, I'm 65 years old and this is al...          5.0\n",
       "3        I use them as shower caps & conditioning caps....          5.0\n",
       "4        This is my go-to daily sunblock. It leaves no ...          5.0\n",
       "...                                                    ...          ...\n",
       "5094302  After watching my Dad struggle with his scisso...          5.0\n",
       "5094303  Like most sound machines, the sounds choices a...          3.0\n",
       "5094304  I bought this product because it indicated 30 ...          5.0\n",
       "5094305  We have used Oral-B products for 15 years; thi...          5.0\n",
       "5094306  I love this toothbrush. It's easy to use, and ...          5.0\n",
       "\n",
       "[5093907 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_and_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ca74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = {1:1, 2:1, 3:2, 4:3, 5:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc0c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the ratings to appropriate classes.\n",
    "\n",
    "reviews_and_ratings.loc[:,\"class\"] = reviews_and_ratings[\"star_rating\"].map(ratings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2dc0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094302</th>\n",
       "      <td>After watching my Dad struggle with his scisso...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094303</th>\n",
       "      <td>Like most sound machines, the sounds choices a...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094304</th>\n",
       "      <td>I bought this product because it indicated 30 ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094305</th>\n",
       "      <td>We have used Oral-B products for 15 years; thi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094306</th>\n",
       "      <td>I love this toothbrush. It's easy to use, and ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating  class\n",
       "0                         Love this, excellent sun block!!          5.0      3\n",
       "1        The great thing about this cream is that it do...          5.0      3\n",
       "2        Great Product, I'm 65 years old and this is al...          5.0      3\n",
       "3        I use them as shower caps & conditioning caps....          5.0      3\n",
       "4        This is my go-to daily sunblock. It leaves no ...          5.0      3\n",
       "...                                                    ...          ...    ...\n",
       "5094302  After watching my Dad struggle with his scisso...          5.0      3\n",
       "5094303  Like most sound machines, the sounds choices a...          3.0      2\n",
       "5094304  I bought this product because it indicated 30 ...          5.0      3\n",
       "5094305  We have used Oral-B products for 15 years; thi...          5.0      3\n",
       "5094306  I love this toothbrush. It's easy to use, and ...          5.0      3\n",
       "\n",
       "[5093907 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_and_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46a64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesisng.\n",
    "\n",
    "# Convert text to lowercase.\n",
    "reviews_and_ratings[\"review_body\"] = reviews_and_ratings[\"review_body\"].str.lower()\n",
    "\n",
    "# Remove HTML tags.\n",
    "reviews_and_ratings[\"review_body\"] = [re.sub('<[^<]+?>', '', str(x)) for x in reviews_and_ratings[\"review_body\"]]\n",
    "\n",
    "# Remove URLs.\n",
    "reviews_and_ratings[\"review_body\"] =  [re.sub(r\"http\\S+\",\"\", str(x)) for x in reviews_and_ratings[\"review_body\"]]\n",
    "\n",
    "# Expand contractions.\n",
    "reviews_and_ratings[\"review_body\"] = [contractions.fix(str(x)) for x in reviews_and_ratings[\"review_body\"]]\n",
    "\n",
    "# Remove non-alphabetical characters.\n",
    "reviews_and_ratings[\"review_body\"] = [re.sub(r\"[^a-zA-Z ]\", \"\", str(x)) for x in reviews_and_ratings[\"review_body\"]]\n",
    "\n",
    "# Remove excess spaces.\n",
    "reviews_and_ratings[\"review_body\"] = reviews_and_ratings[\"review_body\"].replace(\"\\s+\", \" \", regex = True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7722fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the length of each review. \n",
    "\n",
    "reviews_and_ratings[\"review_length\"] = [len(str(x)) for x in reviews_and_ratings[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d04cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop reviews without content.\n",
    "\n",
    "reviews_and_ratings = reviews_and_ratings[reviews_and_ratings[\"review_length\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6de8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 20000 rows from each class.\n",
    "\n",
    "class_1 = reviews_and_ratings[reviews_and_ratings[\"class\"] == 1].sample(20000)\n",
    "class_2 = reviews_and_ratings[reviews_and_ratings[\"class\"] == 2].sample(20000)\n",
    "class_3 = reviews_and_ratings[reviews_and_ratings[\"class\"] == 3].sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32fd3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset = pd.concat([class_1, class_2, class_3], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de5bf1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3055819</th>\n",
       "      <td>i have purchased these before and i was always...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662879</th>\n",
       "      <td>if this works you would not know it from my sk...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443692</th>\n",
       "      <td>very flimsy not great</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395615</th>\n",
       "      <td>this shampoo is very runny i runs through my f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206392</th>\n",
       "      <td>did not work for me at all left flakes in my h...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5042414</th>\n",
       "      <td>i am sure this nightguard is much better than ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535600</th>\n",
       "      <td>love this productit does exactly what it promi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114896</th>\n",
       "      <td>works exactly as advertised and perfect size f...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890940</th>\n",
       "      <td>great thank you</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504270</th>\n",
       "      <td>this is my favorite curling tool i get really ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating  \\\n",
       "3055819  i have purchased these before and i was always...          2.0   \n",
       "1662879  if this works you would not know it from my sk...          2.0   \n",
       "1443692                              very flimsy not great          2.0   \n",
       "4395615  this shampoo is very runny i runs through my f...          1.0   \n",
       "206392   did not work for me at all left flakes in my h...          2.0   \n",
       "...                                                    ...          ...   \n",
       "5042414  i am sure this nightguard is much better than ...          4.0   \n",
       "4535600  love this productit does exactly what it promi...          5.0   \n",
       "2114896  works exactly as advertised and perfect size f...          5.0   \n",
       "1890940                                    great thank you          5.0   \n",
       "2504270  this is my favorite curling tool i get really ...          5.0   \n",
       "\n",
       "         class  review_length  \n",
       "3055819      1            454  \n",
       "1662879      1            498  \n",
       "1443692      1             21  \n",
       "4395615      1            161  \n",
       "206392       1            122  \n",
       "...        ...            ...  \n",
       "5042414      3            380  \n",
       "4535600      3            122  \n",
       "2114896      3            101  \n",
       "1890940      3             15  \n",
       "2504270      3            145  \n",
       "\n",
       "[60000 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdba2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv for local processing.\n",
    "\n",
    "# balanced_dataset.to_csv(\"balanced_dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "130b5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_dataset = pd.read_csv(\"balanced_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860378f",
   "metadata": {},
   "source": [
    "## 2. Word Embedding\n",
    "In this part the of the assignment, you will generate Word2Vec features for the dataset you generated. You can use Gensim library for this purpose. A helpful tutorial is available in the following link: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbd714",
   "metadata": {},
   "source": [
    "**(a) Load the pretrained “word2vec-google-news-300” Word2Vec model and learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using three examples of your own, e.g., King −Man +Woman = Queen or excellent ∼ outstanding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "434273ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Google's Word2Vec model.\n",
    "\n",
    "wv = downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23077242",
   "metadata": {},
   "source": [
    "**Example 1:** **okay** is most similar to **alright**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3953d1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8877537"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay ~ alright\n",
    "wv.similarity(\"okay\", \"alright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67cf4bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alright', 0.8877537250518799),\n",
       " ('ok', 0.8567795753479004),\n",
       " ('OK', 0.7831767201423645),\n",
       " ('yeah', 0.6638472676277161),\n",
       " ('allright', 0.6537948250770569),\n",
       " ('hey', 0.603424608707428),\n",
       " ('say_Hey_feller', 0.5872976779937744),\n",
       " ('anyway', 0.5856852531433105),\n",
       " ('anyways', 0.5801851153373718),\n",
       " ('maybe', 0.5797765851020813)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay ~ alright\n",
    "wv.most_similar(\"okay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b1b8c",
   "metadata": {},
   "source": [
    "**Example 2:** **terrible** is most similar to **horrible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6daf72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92439204"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# terrible ~ horrible\n",
    "wv.similarity(\"terrible\", \"horrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebfa9adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.9243921041488647),\n",
       " ('horrendous', 0.8467271327972412),\n",
       " ('dreadful', 0.8022766709327698),\n",
       " ('awful', 0.7478912472724915),\n",
       " ('horrid', 0.7179027199745178),\n",
       " ('atrocious', 0.689181387424469),\n",
       " ('horrific', 0.6830835342407227),\n",
       " ('bad', 0.6828612089157104),\n",
       " ('appalling', 0.6752808690071106),\n",
       " ('horrible_horrible', 0.6672273278236389)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# terrible ~ horrible\n",
    "wv.most_similar(\"terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387c50c",
   "metadata": {},
   "source": [
    "**Example 3:** **hate** is most similar to **despise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "766b35f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6712518"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hate ~ despise\n",
    "wv.similarity(\"hate\", \"despise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8a1dc70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('despise', 0.6712517142295837),\n",
       " ('Hate', 0.6400399804115295),\n",
       " ('detest', 0.6179037094116211),\n",
       " ('hatred', 0.6156139969825745),\n",
       " ('hating', 0.6103581786155701),\n",
       " ('hates', 0.6091769933700562),\n",
       " ('HATE', 0.6020098328590393),\n",
       " ('dislike', 0.6013234853744507),\n",
       " ('love', 0.600395679473877),\n",
       " ('hated', 0.5922116637229919)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hate ~ despise\n",
    "wv.most_similar(\"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7ff9f",
   "metadata": {},
   "source": [
    "**(b) Train a Word2Vec model using your own dataset. You will use these extracted features in the subsequent questions of this assignment. Set the embedding size to be 300 and the window size to be 13. You can also consider a minimum word count of 9. Check the semantic similarities for the same two examples in part (a). What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better? For the rest of this assignment, use the pretrained “word2vec-googlenews-300” Word2Ve features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f3bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reviews.\n",
    "\n",
    "reviews = balanced_dataset[\"review_body\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4fb4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of sentences to train the custom Word2Vec model.\n",
    "\n",
    "sentences = []\n",
    "for review in reviews:\n",
    "    temp = []\n",
    "    word_list = review.split(\" \")\n",
    "    for word in word_list:\n",
    "        temp.append(word)\n",
    "    sentences.append(temp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3097953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model using the sentences.\n",
    "\n",
    "w2v_model = Word2Vec(sentences = sentences, window = 13, vector_size = 300, min_count = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca0acc",
   "metadata": {},
   "source": [
    "**Example 1:** **okay** vs **alright**\n",
    "\n",
    "Here we can see that **okay** is most similar to **ok** in the new w2v model, this is because the training data of the two models are different, but the next closest word is still **alright**, so our model seems to work well with the first example. We can also see other candidates for okay such as **awesome**, **fantastic**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1276d43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ok', 0.9183574914932251),\n",
       " ('alright', 0.7863861322402954),\n",
       " ('fine', 0.6159654855728149),\n",
       " ('good', 0.5993782877922058),\n",
       " ('awesome', 0.572403609752655),\n",
       " ('great', 0.5683795809745789),\n",
       " ('amazing', 0.5301942229270935),\n",
       " ('awful', 0.49639642238616943),\n",
       " ('decent', 0.4919431805610657),\n",
       " ('fantastic', 0.48155900835990906)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay test\n",
    "w2v_model.wv.most_similar(\"okay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f772f03",
   "metadata": {},
   "source": [
    "**Example 2:** **terrible** vs **horrible**\n",
    "\n",
    "Here we can see that **terrible** is most similar to **horrible** in our newly trained model. Some other candidates are **awful**, **weird**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8fa7a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.8330451250076294),\n",
       " ('awful', 0.7492370009422302),\n",
       " ('wonderful', 0.6612504124641418),\n",
       " ('fantastic', 0.6514009237289429),\n",
       " ('awesome', 0.6283658742904663),\n",
       " ('amazing', 0.624359667301178),\n",
       " ('strange', 0.6114692091941833),\n",
       " ('funny', 0.5741326212882996),\n",
       " ('nasty', 0.5681740045547485),\n",
       " ('disgusting', 0.5679157972335815)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# good test\n",
    "w2v_model.wv.most_similar(\"terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ad6b4",
   "metadata": {},
   "source": [
    "**King − Man + Woman = Queen and excellent ∼ outstanding.**\n",
    "\n",
    "We can see that **excellent** is close to **outstanding**, however, the similarity between **king** and **queen** is very different from that of Google model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "260829ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outstanding', 0.7779552936553955),\n",
       " ('awesome', 0.6842052936553955),\n",
       " ('terrific', 0.6772099137306213),\n",
       " ('affordable', 0.6662511229515076),\n",
       " ('fabulous', 0.6584552526473999),\n",
       " ('fantastic', 0.639548659324646),\n",
       " ('amazing', 0.6251711249351501),\n",
       " ('attractive', 0.5944375991821289),\n",
       " ('inexpensive', 0.5640001893043518),\n",
       " ('great', 0.5562418103218079)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"excellent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5de2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "king_man_woman_custom = w2v_model.wv[\"king\"] - w2v_model.wv[\"man\"] + w2v_model.wv[\"woman\"]\n",
    "king_man_woman_google = wv[\"king\"] - wv[\"man\"] + wv[\"woman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c11316c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06652864"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# queen vs king - man + woman using custom w2v.\n",
    "\n",
    "dot(king_man_woman_custom, w2v_model.wv[\"queen\"]) / (norm(king_man_woman_custom) * norm(w2v_model.wv[\"queen\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29a03603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73005176"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# queen vs king - man + woman using Google's w2v.\n",
    "dot(king_man_woman_google, wv[\"queen\"]) / (norm(king_man_woman_google) * norm(wv[\"queen\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8172bb3",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We can conclude that our model captured the meanings of adjectives nicely because reviews are usually written using lots of adjectives to express opinions. However, when it comes to nouns, our dataset did not have enough instances to cover them. That is why our results for words like **okay** and **terrible** are quite similar to those of the Google model, but results for **king** and **queen** are different. To sum up, Google's Word2Vec was still better at capturing semantic similarities due to their high quality training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d7f90",
   "metadata": {},
   "source": [
    "## 3. Simple models\n",
    "Using the Google pre-trained Word2Vec features, train a single perceptron and an SVM model for the classification problem. For this purpose, use the average Word2Vec vectors for each review as the input feature (x =1NPN i=1Wi for a review with N words). Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of perceptron and SVM models, report two accuracy values Word2Vec and TF-IDF features. What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained 2 Word2Vec features)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55899f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract average Word2Vec vector for each review.\n",
    "# I initialize an empty array of length 300, then add the Word2Vec vectors of each word in the review to it.\n",
    "# Finally, I divide the array by the number of vectors added to it to get the average vector.\n",
    "\n",
    "w2v_features_3 = []\n",
    "for review in reviews:\n",
    "    array = np.zeros(300)\n",
    "    count = 0\n",
    "    words = review.split(\" \")\n",
    "    for word in words:\n",
    "        try:\n",
    "            array = array + wv[word]\n",
    "            count = count + 1\n",
    "        except:\n",
    "            continue\n",
    "    if count == 0:\n",
    "        w2v_features_3.append(array)      \n",
    "    else:\n",
    "        w2v_features_3.append(array/count)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8ac731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TF-IDF features for each review, similar to Homework 1.\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.0001, max_df = 0.5, ngram_range = (1,3))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_dataset[\"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dcf784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy.\n",
    "\n",
    "def accuracy(predictions, true):\n",
    "    n = len(true)\n",
    "    right = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == predictions[i]:\n",
    "            right += 1\n",
    "    return right / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2024bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets.\n",
    "\n",
    "X_w2v = w2v_features_3\n",
    "X_tfidf = tfidf_features\n",
    "y = balanced_dataset[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cc2cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split.\n",
    "\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y, test_size = 0.2)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0868691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron using Word2Vec embeddings.\n",
    "\n",
    "perceptron_w2v = Perceptron(tol=1e-5, alpha = 0.001)\n",
    "perceptron_w2v.fit(X_train_w2v, y_train_w2v)\n",
    "y_pred_perceptron_w2v = perceptron_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "819a58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM using Word2Vec embeddings.\n",
    "\n",
    "svm_w2v = LinearSVC(C = 1.0, tol = 1e-3)\n",
    "svm_w2v.fit(X_train_w2v, y_train_w2v)\n",
    "y_pred_svm_w2v = svm_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cab66251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perceptron using TF-IDF embeddings.\n",
    "\n",
    "perceptron_tfidf = Perceptron(tol=1e-5, alpha = 0.0001)\n",
    "perceptron_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_perceptron_tfidf = perceptron_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98213df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM using TF-IDF embeddings.\n",
    "\n",
    "svm_tfidf = LinearSVC(C = 1.0, tol = 1e-3)\n",
    "svm_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94b7119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Perceptron using Word2Vec vectors: 0.59375\n",
      "Accuracy for SVM using Word2Vec vectors: 0.6615833333333333\n",
      "Accuracy for Perceptron using TF-IDF vectors: 0.6898333333333333\n",
      "Accuracy for SVM using TF-IDF vectors: 0.7235\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Perceptron using Word2Vec vectors:\", accuracy(y_pred_perceptron_w2v, y_test_w2v.values))\n",
    "print(\"Accuracy for SVM using Word2Vec vectors:\", accuracy(y_pred_svm_w2v, y_test_w2v.values))\n",
    "print(\"Accuracy for Perceptron using TF-IDF vectors:\", accuracy(y_pred_perceptron_tfidf, y_test_tfidf.values))\n",
    "print(\"Accuracy for SVM using TF-IDF vectors:\", accuracy(y_pred_svm_tfidf, y_test_tfidf.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d12c93",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "TF-IDF seems to be better than Word2Vec embeddings for this problem. Both Perceptron and SVM showed better accuracy when they were using TF-IDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd71ad",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks\n",
    "Using the Word2Vec features, train a feedforward multilayer perceptron network for classification. Consider a network with two hidden layers, each with 100 and 10 nodes, respectively. You can use cross entropy loss and your own choice for other hyperparamters, e.g., nonlinearity, number of epochs, etc. Part of getting good results is to select suitable values for these hyperparamters. You can also refer to the following tutorial to familiarize yourself: https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist Although the above tutorial is for image data but the concept of training an MLP is very similar to what we want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab5f38",
   "metadata": {},
   "source": [
    "**(a) To generate the input features, use the average Word2Vec vectors similar to the “Simple models” section and train the neural network. Report accuracy values on the testing split for your MLP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f69750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare datasets.\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, transform=None):\n",
    "        self.features = data.feature\n",
    "        self.labels = data.label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "      \n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "daa47ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNN architecture.\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_1 = 100, hidden_2 = 10):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25c97c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets. We will use the average Word2Vec vectors from question 3.\n",
    "\n",
    "X_fnn = w2v_features_3.copy()\n",
    "y = balanced_dataset[\"class\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40330c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fnn, X_test_fnn, y_train_fnn, y_test_fnn = train_test_split(X_fnn, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3658f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_fnn_df = pd.DataFrame(data = {\"feature\": X_train_fnn, \"label\": y_train_fnn}).reset_index(drop = True)\n",
    "test_data_fnn_df = pd.DataFrame(data = {\"feature\": X_test_fnn, \"label\": y_test_fnn}).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7f19116",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_fnn = ReviewDataset(train_data_fnn_df)\n",
    "test_data_fnn = ReviewDataset(test_data_fnn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c392338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_loader_fnn = torch.utils.data.DataLoader(train_data_fnn, batch_size = batch_size)\n",
    "test_loader_fnn = torch.utils.data.DataLoader(test_data_fnn, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bfd8f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create FNN model for 4a.\n",
    "\n",
    "model_4a = FNN(300,100,10)\n",
    "print(model_4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "465a80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function and optimizer.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_4a.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "264af00b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 --- Loss: 1.0961\n",
      "Epoch: 2/50 --- Loss: 1.0869\n",
      "Epoch: 3/50 --- Loss: 1.0288\n",
      "Epoch: 4/50 --- Loss: 0.9164\n",
      "Epoch: 5/50 --- Loss: 0.8287\n",
      "Epoch: 6/50 --- Loss: 0.7701\n",
      "Epoch: 7/50 --- Loss: 0.7741\n",
      "Epoch: 8/50 --- Loss: 0.7767\n",
      "Epoch: 9/50 --- Loss: 0.7408\n",
      "Epoch: 10/50 --- Loss: 0.7850\n",
      "Epoch: 11/50 --- Loss: 0.6860\n",
      "Epoch: 12/50 --- Loss: 0.7953\n",
      "Epoch: 13/50 --- Loss: 0.6870\n",
      "Epoch: 14/50 --- Loss: 0.6639\n",
      "Epoch: 15/50 --- Loss: 0.6513\n",
      "Epoch: 16/50 --- Loss: 0.6745\n",
      "Epoch: 17/50 --- Loss: 0.6266\n",
      "Epoch: 18/50 --- Loss: 0.7288\n",
      "Epoch: 19/50 --- Loss: 0.6388\n",
      "Epoch: 20/50 --- Loss: 0.6918\n",
      "Epoch: 21/50 --- Loss: 0.5951\n",
      "Epoch: 22/50 --- Loss: 0.6334\n",
      "Epoch: 23/50 --- Loss: 0.6329\n",
      "Epoch: 24/50 --- Loss: 0.6767\n",
      "Epoch: 25/50 --- Loss: 0.6488\n",
      "Epoch: 26/50 --- Loss: 0.6440\n",
      "Epoch: 27/50 --- Loss: 0.5709\n",
      "Epoch: 28/50 --- Loss: 0.6340\n",
      "Epoch: 29/50 --- Loss: 0.6269\n",
      "Epoch: 30/50 --- Loss: 0.5909\n",
      "Epoch: 31/50 --- Loss: 0.6136\n",
      "Epoch: 32/50 --- Loss: 0.5522\n",
      "Epoch: 33/50 --- Loss: 0.6351\n",
      "Epoch: 34/50 --- Loss: 0.5892\n",
      "Epoch: 35/50 --- Loss: 0.6278\n",
      "Epoch: 36/50 --- Loss: 0.6496\n",
      "Epoch: 37/50 --- Loss: 0.7316\n",
      "Epoch: 38/50 --- Loss: 0.6359\n",
      "Epoch: 39/50 --- Loss: 0.7289\n",
      "Epoch: 40/50 --- Loss: 0.5386\n",
      "Epoch: 41/50 --- Loss: 0.6284\n",
      "Epoch: 42/50 --- Loss: 0.5025\n",
      "Epoch: 43/50 --- Loss: 0.6688\n",
      "Epoch: 44/50 --- Loss: 0.6125\n",
      "Epoch: 45/50 --- Loss: 0.5285\n",
      "Epoch: 46/50 --- Loss: 0.5508\n",
      "Epoch: 47/50 --- Loss: 0.5792\n",
      "Epoch: 48/50 --- Loss: 0.5902\n",
      "Epoch: 49/50 --- Loss: 0.5889\n",
      "Epoch: 50/50 --- Loss: 0.6043\n"
     ]
    }
   ],
   "source": [
    "# Model training.\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for data, target in train_loader_fnn:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(torch.float32)\n",
    "        output = model_4a(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch: {}/{} --- Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "649d3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions.\n",
    "\n",
    "prediction_list = []\n",
    "for i, batch in enumerate(test_loader_fnn):\n",
    "    batch[0] = batch[0].to(torch.float32)\n",
    "    output = model_4a(batch[0])\n",
    "    _, predicted = torch.max(output.data, 1) \n",
    "    prediction_list = prediction_list + list(predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8592838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FNN using average Word2Vec vectors: 0.6564166666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for FNN using average Word2Vec vectors:\", accuracy(prediction_list, y_test_fnn.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d307a03",
   "metadata": {},
   "source": [
    "**(b) To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature (x = [WT 1 , ...,WT 10]) and train the neural network. Report the accuracy value on the testing split for your MLP model. What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "128ca086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append first 10 Word2Vec vectors.\n",
    "# Same procedure as question 3, however, we are not going to divide by the total number of words.\n",
    "\n",
    "w2v_features_4b = []\n",
    "for review in reviews:\n",
    "    count = 0\n",
    "    words = review.split(\" \")\n",
    "    for word in words:\n",
    "        if count == 0:\n",
    "            try:\n",
    "                array = [wv[word]]\n",
    "                count = count + 1\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                array = np.concatenate((array, [wv[word]]), axis = 0)\n",
    "                count = count + 1\n",
    "                if count == 10:\n",
    "                    w2v_features_4b.append(array.flatten(\"F\"))\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    # If some words in the review are not in the Word2Vec corpus, or if a review length is shorter than 10, we pad the final array.\n",
    "    \n",
    "    if count == 0:\n",
    "        array = [np.zeros(300)]\n",
    "        for i in range(count, 9):\n",
    "            array = np.concatenate((array, [np.zeros(300)]), axis = 0)\n",
    "        w2v_features_4b.append(array.flatten(\"F\"))\n",
    "    else:  \n",
    "        if count < 10:\n",
    "            for i in range(count, 10):\n",
    "                array = np.concatenate((array, [np.zeros(300)]), axis = 0)\n",
    "            w2v_features_4b.append(array.flatten(\"F\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f67f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets.\n",
    "\n",
    "X_fnn = w2v_features_4b\n",
    "y = balanced_dataset[\"class\"] - 1\n",
    "X_train_fnn, X_test_fnn, y_train_fnn, y_test_fnn = train_test_split(X_fnn, y, test_size = 0.2)\n",
    "train_data_fnn = pd.DataFrame(data = {\"feature\": X_train_fnn, \"label\": y_train_fnn}).reset_index(drop = True)\n",
    "test_data_fnn = pd.DataFrame(data = {\"feature\": X_test_fnn, \"label\": y_test_fnn}).reset_index(drop = True)\n",
    "train_data_fnn = ReviewDataset(train_data_fnn)\n",
    "test_data_fnn = ReviewDataset(test_data_fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "984cc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_loader_fnn = torch.utils.data.DataLoader(train_data_fnn, batch_size = batch_size)\n",
    "test_loader_fnn = torch.utils.data.DataLoader(test_data_fnn, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45cda9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create FNN model for 4b.\n",
    "\n",
    "model_4b = FNN(3000,100,10)\n",
    "print(model_4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eec55c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function and optimizer.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_4b.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5c336dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 --- Loss: 1.0317\n",
      "Epoch: 2/50 --- Loss: 1.0071\n",
      "Epoch: 3/50 --- Loss: 0.9900\n",
      "Epoch: 4/50 --- Loss: 0.7967\n",
      "Epoch: 5/50 --- Loss: 0.8089\n",
      "Epoch: 6/50 --- Loss: 0.8250\n",
      "Epoch: 7/50 --- Loss: 0.6987\n",
      "Epoch: 8/50 --- Loss: 0.7625\n",
      "Epoch: 9/50 --- Loss: 0.6885\n",
      "Epoch: 10/50 --- Loss: 0.6945\n",
      "Epoch: 11/50 --- Loss: 0.6529\n",
      "Epoch: 12/50 --- Loss: 0.6522\n",
      "Epoch: 13/50 --- Loss: 0.6158\n",
      "Epoch: 14/50 --- Loss: 0.4772\n",
      "Epoch: 15/50 --- Loss: 0.5560\n",
      "Epoch: 16/50 --- Loss: 0.5966\n",
      "Epoch: 17/50 --- Loss: 0.4544\n",
      "Epoch: 18/50 --- Loss: 0.4222\n",
      "Epoch: 19/50 --- Loss: 0.4072\n",
      "Epoch: 20/50 --- Loss: 0.5729\n",
      "Epoch: 21/50 --- Loss: 0.4044\n",
      "Epoch: 22/50 --- Loss: 0.5240\n",
      "Epoch: 23/50 --- Loss: 0.2939\n",
      "Epoch: 24/50 --- Loss: 0.3442\n",
      "Epoch: 25/50 --- Loss: 0.2689\n",
      "Epoch: 26/50 --- Loss: 0.3420\n",
      "Epoch: 27/50 --- Loss: 0.1792\n",
      "Epoch: 28/50 --- Loss: 0.2317\n",
      "Epoch: 29/50 --- Loss: 0.6208\n",
      "Epoch: 30/50 --- Loss: 0.3414\n",
      "Epoch: 31/50 --- Loss: 0.2970\n",
      "Epoch: 32/50 --- Loss: 0.2161\n",
      "Epoch: 33/50 --- Loss: 0.3045\n",
      "Epoch: 34/50 --- Loss: 0.1351\n",
      "Epoch: 35/50 --- Loss: 0.1616\n",
      "Epoch: 36/50 --- Loss: 0.3714\n",
      "Epoch: 37/50 --- Loss: 0.1232\n",
      "Epoch: 38/50 --- Loss: 0.3104\n",
      "Epoch: 39/50 --- Loss: 0.0959\n",
      "Epoch: 40/50 --- Loss: 0.0808\n",
      "Epoch: 41/50 --- Loss: 0.1721\n",
      "Epoch: 42/50 --- Loss: 0.1902\n",
      "Epoch: 43/50 --- Loss: 0.2261\n",
      "Epoch: 44/50 --- Loss: 0.1450\n",
      "Epoch: 45/50 --- Loss: 0.1612\n",
      "Epoch: 46/50 --- Loss: 0.0756\n",
      "Epoch: 47/50 --- Loss: 0.0589\n",
      "Epoch: 48/50 --- Loss: 0.1368\n",
      "Epoch: 49/50 --- Loss: 0.0704\n",
      "Epoch: 50/50 --- Loss: 0.0427\n"
     ]
    }
   ],
   "source": [
    "# Model training.\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for data, target in train_loader_fnn:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(torch.float32)\n",
    "        output = model_4b(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    \n",
    "    print('Epoch: {}/{} --- Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9dfd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions.\n",
    "\n",
    "prediction_list = []\n",
    "for i, batch in enumerate(test_loader_fnn):\n",
    "    batch[0] = batch[0].to(torch.float32)\n",
    "    output = model_4b(batch[0])\n",
    "    _, predicted = torch.max(output.data, 1) \n",
    "    prediction_list = prediction_list + list(predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94741bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FNN using 10 first Word2Vec vectors: 0.5546666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for FNN using 10 first Word2Vec vectors:\", accuracy(prediction_list, y_test_fnn.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949e314",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "MLP using the average Word2Vec vectors performed quite similarly to the simple models in part 3 (~ 0.65 accuracy). However, MLP using the concateneated Word2Vec vectors underperformed in this situation (~ 0.55 accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fc266",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks\n",
    "Using the Word2Vec features, train a recurrent neural network (RNN) for classification. You can refer to the following tutorial to familiarize yourself: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad60f6",
   "metadata": {},
   "source": [
    "**(a) Train a simple RNN for sentiment analysis. You can consider an RNN cell with the hidden state size of 20. To feed your data into our RNN, limit the maximum review length to 20 by truncating longer reviews and padding shorter reviews with a null value (0). Report accuracy values on the testing split for your RNN model. What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6befca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec features with lenght 20 reviews.\n",
    "# Samme procedure as question 4, except instead of length 10 we are considering length 20.\n",
    "\n",
    "w2v_features_5 = []\n",
    "for review in reviews:\n",
    "    count = 0\n",
    "    words = review.split(\" \")\n",
    "    for word in words:\n",
    "        if count == 0:\n",
    "            try:\n",
    "                array = [wv[word]]\n",
    "                count = count + 1\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                array = np.concatenate((array, [wv[word]]), axis = 0)\n",
    "                count = count + 1\n",
    "                if count == 20:\n",
    "                    w2v_features_5.append(array)\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    # If some words in the review are not in the Word2Vec corpus, or if a review length is shorter than 20, we pad the final array.           \n",
    "    \n",
    "    if count == 0:\n",
    "        array = [np.zeros(300)]\n",
    "        for i in range(1, 20):\n",
    "            array = np.concatenate((array, [np.zeros(300)]), axis = 0)\n",
    "        w2v_features_5.append(array)\n",
    "    else:\n",
    "        if count < 20:\n",
    "            for i in range(count, 20):\n",
    "                array = np.concatenate((array, [np.zeros(300)]), axis = 0)\n",
    "            w2v_features_5.append(array)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d0bc1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN architecture.\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f4bdb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets.\n",
    "\n",
    "X_5 = w2v_features_5\n",
    "y = balanced_dataset[\"class\"] - 1\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(X_5, y, test_size = 0.2)\n",
    "train_data_5_df = pd.DataFrame(data = {\"feature\": X_train_5, \"label\": y_train_5}).reset_index(drop = True)\n",
    "test_data_5_df = pd.DataFrame(data = {\"feature\": X_test_5, \"label\": y_test_5}).reset_index(drop = True)\n",
    "train_data_5 = ReviewDataset(train_data_5_df)\n",
    "test_data_5 = ReviewDataset(test_data_5_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23e304e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_data_5, batch_size = batch_size)\n",
    "test_loader_5 = torch.utils.data.DataLoader(test_data_5, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ead76fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN model for 5a.\n",
    "\n",
    "model_5a = RNN(300, 3, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22bc77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function and optimizer.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_5a.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ee30742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 --- Loss: 0.9935\n",
      "Epoch: 2/50 --- Loss: 0.9193\n",
      "Epoch: 3/50 --- Loss: 0.9254\n",
      "Epoch: 4/50 --- Loss: 0.7959\n",
      "Epoch: 5/50 --- Loss: 0.8243\n",
      "Epoch: 6/50 --- Loss: 0.7699\n",
      "Epoch: 7/50 --- Loss: 0.6954\n",
      "Epoch: 8/50 --- Loss: 0.7400\n",
      "Epoch: 9/50 --- Loss: 0.7296\n",
      "Epoch: 10/50 --- Loss: 0.7507\n",
      "Epoch: 11/50 --- Loss: 0.7669\n",
      "Epoch: 12/50 --- Loss: 0.6341\n",
      "Epoch: 13/50 --- Loss: 0.7504\n",
      "Epoch: 14/50 --- Loss: 0.7795\n",
      "Epoch: 15/50 --- Loss: 0.6566\n",
      "Epoch: 16/50 --- Loss: 0.7123\n",
      "Epoch: 17/50 --- Loss: 0.7696\n",
      "Epoch: 18/50 --- Loss: 0.6848\n",
      "Epoch: 19/50 --- Loss: 0.6781\n",
      "Epoch: 20/50 --- Loss: 0.7901\n",
      "Epoch: 21/50 --- Loss: 0.6840\n",
      "Epoch: 22/50 --- Loss: 0.5969\n",
      "Epoch: 23/50 --- Loss: 0.5553\n",
      "Epoch: 24/50 --- Loss: 0.7223\n",
      "Epoch: 25/50 --- Loss: 0.6925\n",
      "Epoch: 26/50 --- Loss: 0.7051\n",
      "Epoch: 27/50 --- Loss: 0.8062\n",
      "Epoch: 28/50 --- Loss: 0.6234\n",
      "Epoch: 29/50 --- Loss: 0.6020\n",
      "Epoch: 30/50 --- Loss: 0.6387\n",
      "Epoch: 31/50 --- Loss: 0.9872\n",
      "Epoch: 32/50 --- Loss: 0.6990\n",
      "Epoch: 33/50 --- Loss: 0.7559\n",
      "Epoch: 34/50 --- Loss: 0.5985\n",
      "Epoch: 35/50 --- Loss: 0.6191\n",
      "Epoch: 36/50 --- Loss: 0.5630\n",
      "Epoch: 37/50 --- Loss: 0.6748\n",
      "Epoch: 38/50 --- Loss: 0.6070\n",
      "Epoch: 39/50 --- Loss: 0.7756\n",
      "Epoch: 40/50 --- Loss: 0.7846\n",
      "Epoch: 41/50 --- Loss: 0.6263\n",
      "Epoch: 42/50 --- Loss: 0.9077\n",
      "Epoch: 43/50 --- Loss: 0.7504\n",
      "Epoch: 44/50 --- Loss: 0.6752\n",
      "Epoch: 45/50 --- Loss: 0.9100\n",
      "Epoch: 46/50 --- Loss: 0.8026\n",
      "Epoch: 47/50 --- Loss: 0.6536\n",
      "Epoch: 48/50 --- Loss: 0.7944\n",
      "Epoch: 49/50 --- Loss: 0.8209\n",
      "Epoch: 50/50 --- Loss: 0.7573\n"
     ]
    }
   ],
   "source": [
    "# Model training.\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for data, target in train_loader_5:\n",
    "        optimizer.zero_grad() \n",
    "        data = data.to(torch.float32)\n",
    "        output, hidden = model_5a(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "    print('Epoch: {}/{} --- Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e790240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions.\n",
    "\n",
    "prediction_list = []\n",
    "for i, batch in enumerate(test_loader_5):\n",
    "    batch[0] = batch[0].to(torch.float32)\n",
    "    output, hidden = model_5a(batch[0])\n",
    "    _, predicted = torch.max(output.data, 1) \n",
    "    prediction_list = prediction_list + list(predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a91a68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RNN using 20 first Word2Vec vectors: 0.61175\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for RNN using 20 first Word2Vec vectors:\", accuracy(prediction_list, y_test_5.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f86257",
   "metadata": {},
   "source": [
    "**(b) Repeat part (a) by considering a gated recurrent unit cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bca4e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU architecture.\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5ae1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRU model for 5b.\n",
    "model_5b = GRU(300, 3, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ffa7c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function and optimizer.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_5b.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "234f5b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 --- Loss: 0.6357\n",
      "Epoch: 2/50 --- Loss: 0.5890\n",
      "Epoch: 3/50 --- Loss: 0.5727\n",
      "Epoch: 4/50 --- Loss: 0.5622\n",
      "Epoch: 5/50 --- Loss: 0.5531\n",
      "Epoch: 6/50 --- Loss: 0.5423\n",
      "Epoch: 7/50 --- Loss: 0.5303\n",
      "Epoch: 8/50 --- Loss: 0.5204\n",
      "Epoch: 9/50 --- Loss: 0.5144\n",
      "Epoch: 10/50 --- Loss: 0.5118\n",
      "Epoch: 11/50 --- Loss: 0.5123\n",
      "Epoch: 12/50 --- Loss: 0.5147\n",
      "Epoch: 13/50 --- Loss: 0.5187\n",
      "Epoch: 14/50 --- Loss: 0.5244\n",
      "Epoch: 15/50 --- Loss: 0.5268\n",
      "Epoch: 16/50 --- Loss: 0.5341\n",
      "Epoch: 17/50 --- Loss: 0.5376\n",
      "Epoch: 18/50 --- Loss: 0.5426\n",
      "Epoch: 19/50 --- Loss: 0.5202\n",
      "Epoch: 20/50 --- Loss: 0.5407\n",
      "Epoch: 21/50 --- Loss: 0.5474\n",
      "Epoch: 22/50 --- Loss: 0.5436\n",
      "Epoch: 23/50 --- Loss: 0.5435\n",
      "Epoch: 24/50 --- Loss: 0.5650\n",
      "Epoch: 25/50 --- Loss: 0.5520\n",
      "Epoch: 26/50 --- Loss: 0.6068\n",
      "Epoch: 27/50 --- Loss: 0.5752\n",
      "Epoch: 28/50 --- Loss: 0.5879\n",
      "Epoch: 29/50 --- Loss: 0.5866\n",
      "Epoch: 30/50 --- Loss: 0.6010\n",
      "Epoch: 31/50 --- Loss: 0.5879\n",
      "Epoch: 32/50 --- Loss: 0.6194\n",
      "Epoch: 33/50 --- Loss: 0.6069\n",
      "Epoch: 34/50 --- Loss: 0.6163\n",
      "Epoch: 35/50 --- Loss: 0.6032\n",
      "Epoch: 36/50 --- Loss: 0.6097\n",
      "Epoch: 37/50 --- Loss: 0.5870\n",
      "Epoch: 38/50 --- Loss: 0.6098\n",
      "Epoch: 39/50 --- Loss: 0.6132\n",
      "Epoch: 40/50 --- Loss: 0.6496\n",
      "Epoch: 41/50 --- Loss: 0.6096\n",
      "Epoch: 42/50 --- Loss: 0.6333\n",
      "Epoch: 43/50 --- Loss: 0.6438\n",
      "Epoch: 44/50 --- Loss: 0.6503\n",
      "Epoch: 45/50 --- Loss: 0.6100\n",
      "Epoch: 46/50 --- Loss: 0.6252\n",
      "Epoch: 47/50 --- Loss: 0.6058\n",
      "Epoch: 48/50 --- Loss: 0.6506\n",
      "Epoch: 49/50 --- Loss: 0.6443\n",
      "Epoch: 50/50 --- Loss: 0.6501\n"
     ]
    }
   ],
   "source": [
    "# Model training.\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for data, target in train_loader_5:\n",
    "        optimizer.zero_grad() \n",
    "        data = data.to(torch.float32)\n",
    "        output, hidden = model_5b(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "    print('Epoch: {}/{} --- Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d370ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions.\n",
    "\n",
    "prediction_list = []\n",
    "for i, batch in enumerate(test_loader_5):\n",
    "    batch[0] = batch[0].to(torch.float32)\n",
    "    output, hidden = model_5b(batch[0])\n",
    "    _, predicted = torch.max(output.data, 1) \n",
    "    prediction_list = prediction_list + list(predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cb67b9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GRU using 20 first Word2Vec vectors: 0.6403333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for GRU using 20 first Word2Vec vectors:\", accuracy(prediction_list, y_test_5.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4965b236",
   "metadata": {},
   "source": [
    "**(c) Repeat part (a) by considering an LSTM unit cell. What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e36dc908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM architecture.\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        cell = self.init_cell(batch_size)\n",
    "        out, hidden = self.lstm(x, (hidden, cell))\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def init_cell(self, batch_size):\n",
    "        cell = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        \n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "095ab97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5c = LSTM(300, 3, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e11e9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function and optimizer.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_5c.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcc324ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 --- Loss: 0.7002\n",
      "Epoch: 2/50 --- Loss: 0.5616\n",
      "Epoch: 3/50 --- Loss: 0.5159\n",
      "Epoch: 4/50 --- Loss: 0.4927\n",
      "Epoch: 5/50 --- Loss: 0.4783\n",
      "Epoch: 6/50 --- Loss: 0.4713\n",
      "Epoch: 7/50 --- Loss: 0.4669\n",
      "Epoch: 8/50 --- Loss: 0.4601\n",
      "Epoch: 9/50 --- Loss: 0.4634\n",
      "Epoch: 10/50 --- Loss: 0.4657\n",
      "Epoch: 11/50 --- Loss: 0.4620\n",
      "Epoch: 12/50 --- Loss: 0.4631\n",
      "Epoch: 13/50 --- Loss: 0.4890\n",
      "Epoch: 14/50 --- Loss: 0.5002\n",
      "Epoch: 15/50 --- Loss: 0.4838\n",
      "Epoch: 16/50 --- Loss: 0.5322\n",
      "Epoch: 17/50 --- Loss: 0.5346\n",
      "Epoch: 18/50 --- Loss: 0.5774\n",
      "Epoch: 19/50 --- Loss: 0.5292\n",
      "Epoch: 20/50 --- Loss: 0.5185\n",
      "Epoch: 21/50 --- Loss: 0.5793\n",
      "Epoch: 22/50 --- Loss: 0.5048\n",
      "Epoch: 23/50 --- Loss: 0.6153\n",
      "Epoch: 24/50 --- Loss: 0.5379\n",
      "Epoch: 25/50 --- Loss: 0.5040\n",
      "Epoch: 26/50 --- Loss: 0.5810\n",
      "Epoch: 27/50 --- Loss: 0.5286\n",
      "Epoch: 28/50 --- Loss: 0.5338\n",
      "Epoch: 29/50 --- Loss: 0.5289\n",
      "Epoch: 30/50 --- Loss: 0.5964\n",
      "Epoch: 31/50 --- Loss: 0.4849\n",
      "Epoch: 32/50 --- Loss: 0.5416\n",
      "Epoch: 33/50 --- Loss: 0.4779\n",
      "Epoch: 34/50 --- Loss: 0.4941\n",
      "Epoch: 35/50 --- Loss: 0.5734\n",
      "Epoch: 36/50 --- Loss: 0.5993\n",
      "Epoch: 37/50 --- Loss: 0.5761\n",
      "Epoch: 38/50 --- Loss: 0.5774\n",
      "Epoch: 39/50 --- Loss: 0.5553\n",
      "Epoch: 40/50 --- Loss: 0.5568\n",
      "Epoch: 41/50 --- Loss: 0.6009\n",
      "Epoch: 42/50 --- Loss: 0.5959\n",
      "Epoch: 43/50 --- Loss: 0.6301\n",
      "Epoch: 44/50 --- Loss: 0.6177\n",
      "Epoch: 45/50 --- Loss: 0.7080\n",
      "Epoch: 46/50 --- Loss: 0.5930\n",
      "Epoch: 47/50 --- Loss: 0.6443\n",
      "Epoch: 48/50 --- Loss: 0.5760\n",
      "Epoch: 49/50 --- Loss: 0.5711\n",
      "Epoch: 50/50 --- Loss: 0.6213\n"
     ]
    }
   ],
   "source": [
    "# Model training.\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    for data, target in train_loader_5:\n",
    "        optimizer.zero_grad() \n",
    "        data = data.to(torch.float32)\n",
    "        output, hidden = model_5c(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        \n",
    "    print('Epoch: {}/{} --- Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5411794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions.\n",
    "\n",
    "prediction_list = []\n",
    "for i, batch in enumerate(test_loader_5):\n",
    "    batch[0] = batch[0].to(torch.float32)\n",
    "    output, hidden = model_5c(batch[0])\n",
    "    _, predicted = torch.max(output.data, 1) \n",
    "    prediction_list = prediction_list + list(predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a680528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LSTM using 20 first Word2Vec vectors: 0.6334166666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for LSTM using 20 first Word2Vec vectors:\", accuracy(prediction_list, y_test_5.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd6a986",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We can see that GRU and LSTM performed better than RNN when it comes to this classification problem. The hidden states and cell states helped improved the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562405f4",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb03e7",
   "metadata": {},
   "source": [
    "**Feedforward Neural Networks**: https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "\n",
    "**Recurrent Neural Networks**:https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
