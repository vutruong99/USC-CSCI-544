{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet = True)\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('averaged_perceptron_tagger', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data while skipping bad lines.\n",
    "\n",
    "dataframe = pd.read_table(\"amazon_reviews_us_Beauty_v1_00.tsv\", error_bad_lines = False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>1797882</td>\n",
       "      <td>R3I2DHQBR577SS</td>\n",
       "      <td>B001ANOOOE</td>\n",
       "      <td>2102612</td>\n",
       "      <td>The Naked Bee Vitmin C Moisturizing Sunscreen ...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>18381298</td>\n",
       "      <td>R1QNE9NQFJC2Y4</td>\n",
       "      <td>B0016J22EQ</td>\n",
       "      <td>106393691</td>\n",
       "      <td>Alba Botanica Sunless Tanning Lotion, 4 Ounce</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Thank you Alba Bontanica!</td>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>19242472</td>\n",
       "      <td>R3LIDG2Q4LJBAO</td>\n",
       "      <td>B00HU6UQAG</td>\n",
       "      <td>375449471</td>\n",
       "      <td>Elysee Infusion Skin Therapy Elixir, 2oz.</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>19551372</td>\n",
       "      <td>R3KSZHPAEVPEAL</td>\n",
       "      <td>B002HWS7RM</td>\n",
       "      <td>255651889</td>\n",
       "      <td>Diane D722 Color, Perm And Conditioner Process...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>GOOD DEAL!</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>14802407</td>\n",
       "      <td>RAI2OIG50KZ43</td>\n",
       "      <td>B00SM99KWU</td>\n",
       "      <td>116158747</td>\n",
       "      <td>Biore UV Aqua Rich Watery Essence SPF50+/PA+++...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>this soaks in quick and provides a nice base f...</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094302</th>\n",
       "      <td>US</td>\n",
       "      <td>50113639</td>\n",
       "      <td>RZ7RZ02MTP4SL</td>\n",
       "      <td>B000050B70</td>\n",
       "      <td>185454094</td>\n",
       "      <td>Conair NE150NSCS Cordless Nose and Ear Hair Tr...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great Little Grooming Tool</td>\n",
       "      <td>After watching my Dad struggle with his scisso...</td>\n",
       "      <td>2000-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094303</th>\n",
       "      <td>US</td>\n",
       "      <td>52940456</td>\n",
       "      <td>R2IRC0IZ8YCE5T</td>\n",
       "      <td>B000050FF2</td>\n",
       "      <td>678848064</td>\n",
       "      <td>Homedics Envirascape Sound Spa Alarm Clock Radio</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not bad for the price</td>\n",
       "      <td>Like most sound machines, the sounds choices a...</td>\n",
       "      <td>2000-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094304</th>\n",
       "      <td>US</td>\n",
       "      <td>47587881</td>\n",
       "      <td>R1U4ZSXOD228CZ</td>\n",
       "      <td>B000050B6U</td>\n",
       "      <td>862195513</td>\n",
       "      <td>Conair Instant Heat Curling Iron</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Best Curling Iron Ever</td>\n",
       "      <td>I bought this product because it indicated 30 ...</td>\n",
       "      <td>2000-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094305</th>\n",
       "      <td>US</td>\n",
       "      <td>53047750</td>\n",
       "      <td>R3SFJLZE09URWM</td>\n",
       "      <td>B000050FDE</td>\n",
       "      <td>195242894</td>\n",
       "      <td>Oral-B Professional Care 1000 Power Toothbrush</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>The best electric toothbrush ever, REALLY!</td>\n",
       "      <td>We have used Oral-B products for 15 years; thi...</td>\n",
       "      <td>2000-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094306</th>\n",
       "      <td>US</td>\n",
       "      <td>51193940</td>\n",
       "      <td>R1MEWK4I7YS5XK</td>\n",
       "      <td>B000050AUD</td>\n",
       "      <td>190668305</td>\n",
       "      <td>Sonicare PL-4 (4700) Sonic Toothbrush</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Smooth and shiny teeth!</td>\n",
       "      <td>I love this toothbrush. It's easy to use, and ...</td>\n",
       "      <td>2000-10-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5094307 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0                US      1797882  R3I2DHQBR577SS  B001ANOOOE         2102612   \n",
       "1                US     18381298  R1QNE9NQFJC2Y4  B0016J22EQ       106393691   \n",
       "2                US     19242472  R3LIDG2Q4LJBAO  B00HU6UQAG       375449471   \n",
       "3                US     19551372  R3KSZHPAEVPEAL  B002HWS7RM       255651889   \n",
       "4                US     14802407   RAI2OIG50KZ43  B00SM99KWU       116158747   \n",
       "...             ...          ...             ...         ...             ...   \n",
       "5094302          US     50113639   RZ7RZ02MTP4SL  B000050B70       185454094   \n",
       "5094303          US     52940456  R2IRC0IZ8YCE5T  B000050FF2       678848064   \n",
       "5094304          US     47587881  R1U4ZSXOD228CZ  B000050B6U       862195513   \n",
       "5094305          US     53047750  R3SFJLZE09URWM  B000050FDE       195242894   \n",
       "5094306          US     51193940  R1MEWK4I7YS5XK  B000050AUD       190668305   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0        The Naked Bee Vitmin C Moisturizing Sunscreen ...           Beauty   \n",
       "1            Alba Botanica Sunless Tanning Lotion, 4 Ounce           Beauty   \n",
       "2                Elysee Infusion Skin Therapy Elixir, 2oz.           Beauty   \n",
       "3        Diane D722 Color, Perm And Conditioner Process...           Beauty   \n",
       "4        Biore UV Aqua Rich Watery Essence SPF50+/PA+++...           Beauty   \n",
       "...                                                    ...              ...   \n",
       "5094302  Conair NE150NSCS Cordless Nose and Ear Hair Tr...           Beauty   \n",
       "5094303   Homedics Envirascape Sound Spa Alarm Clock Radio           Beauty   \n",
       "5094304                   Conair Instant Heat Curling Iron           Beauty   \n",
       "5094305     Oral-B Professional Care 1000 Power Toothbrush           Beauty   \n",
       "5094306              Sonicare PL-4 (4700) Sonic Toothbrush           Beauty   \n",
       "\n",
       "        star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0                 5            0.0          0.0    N                 Y   \n",
       "1                 5            0.0          0.0    N                 Y   \n",
       "2                 5            0.0          0.0    N                 Y   \n",
       "3                 5            0.0          0.0    N                 Y   \n",
       "4                 5            0.0          0.0    N                 Y   \n",
       "...             ...            ...          ...  ...               ...   \n",
       "5094302           5           10.0         10.0    N                 N   \n",
       "5094303           3           23.0         23.0    N                 N   \n",
       "5094304           5           89.0         97.0    N                 N   \n",
       "5094305           5           10.0         10.0    N                 N   \n",
       "5094306           5           23.0         23.0    N                 N   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1                                Thank you Alba Bontanica!   \n",
       "2                                               Five Stars   \n",
       "3                                               GOOD DEAL!   \n",
       "4        this soaks in quick and provides a nice base f...   \n",
       "...                                                    ...   \n",
       "5094302                         Great Little Grooming Tool   \n",
       "5094303                              Not bad for the price   \n",
       "5094304                             Best Curling Iron Ever   \n",
       "5094305         The best electric toothbrush ever, REALLY!   \n",
       "5094306                            Smooth and shiny teeth!   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                         Love this, excellent sun block!!  2015-08-31  \n",
       "1        The great thing about this cream is that it do...  2015-08-31  \n",
       "2        Great Product, I'm 65 years old and this is al...  2015-08-31  \n",
       "3        I use them as shower caps & conditioning caps....  2015-08-31  \n",
       "4        This is my go-to daily sunblock. It leaves no ...  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "5094302  After watching my Dad struggle with his scisso...  2000-11-12  \n",
       "5094303  Like most sound machines, the sounds choices a...  2000-11-07  \n",
       "5094304  I bought this product because it indicated 30 ...  2000-11-02  \n",
       "5094305  We have used Oral-B products for 15 years; thi...  2000-11-01  \n",
       "5094306  I love this toothbrush. It's easy to use, and ...  2000-10-31  \n",
       "\n",
       "[5094307 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_and_ratings = dataframe[[\"review_body\", \"star_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to numerical values (\"1.0\" -> 1.0) and mark as None if the value is invalid.\n",
    "\n",
    "reviews_and_ratings.loc[: , \"star_rating\"] = pd.to_numeric(reviews_and_ratings[\"star_rating\"], errors = 'coerce', downcast = 'integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values.\n",
    "\n",
    "reviews_and_ratings = reviews_and_ratings.dropna(how = \"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094302</th>\n",
       "      <td>After watching my Dad struggle with his scisso...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094303</th>\n",
       "      <td>Like most sound machines, the sounds choices a...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094304</th>\n",
       "      <td>I bought this product because it indicated 30 ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094305</th>\n",
       "      <td>We have used Oral-B products for 15 years; thi...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094306</th>\n",
       "      <td>I love this toothbrush. It's easy to use, and ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093907 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating\n",
       "0                         Love this, excellent sun block!!          5.0\n",
       "1        The great thing about this cream is that it do...          5.0\n",
       "2        Great Product, I'm 65 years old and this is al...          5.0\n",
       "3        I use them as shower caps & conditioning caps....          5.0\n",
       "4        This is my go-to daily sunblock. It leaves no ...          5.0\n",
       "...                                                    ...          ...\n",
       "5094302  After watching my Dad struggle with his scisso...          5.0\n",
       "5094303  Like most sound machines, the sounds choices a...          3.0\n",
       "5094304  I bought this product because it indicated 30 ...          5.0\n",
       "5094305  We have used Oral-B products for 15 years; thi...          5.0\n",
       "5094306  I love this toothbrush. It's easy to use, and ...          5.0\n",
       "\n",
       "[5093907 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_and_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = {1:1, 2:1, 3:2, 4:3, 5:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map the ratings to appropriate classes.\n",
    "\n",
    "reviews_and_ratings.loc[:,\"class\"] = reviews_and_ratings[\"star_rating\"].map(ratings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 20000 rows from each class.\n",
    "\n",
    "class_1 = reviews_and_ratings[reviews_and_ratings[\"class\"] == 1].sample(20000)\n",
    "class_2 = reviews_and_ratings[reviews_and_ratings[\"class\"] == 2].sample(20000)\n",
    "class_3 = reviews_and_ratings[reviews_and_ratings[\"class\"] == 3].sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset = pd.concat([class_1, class_2, class_3], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4965866</th>\n",
       "      <td>You'll need heavy duty shears to open packages...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225032</th>\n",
       "      <td>It didn't take the brassiness out of my hair a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826255</th>\n",
       "      <td>I have a suspicion that this brush may be the ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4812345</th>\n",
       "      <td>I was so thrilled to find a cologne that has b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084719</th>\n",
       "      <td>While the toothbrush does an excellent job cle...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386104</th>\n",
       "      <td>The picture is deceptive in that you only get ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127620</th>\n",
       "      <td>Most of these colors are very natural. A lot o...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404233</th>\n",
       "      <td>it does work.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279257</th>\n",
       "      <td>Exactly what I expected</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552877</th>\n",
       "      <td>Love the fragrance</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating  class\n",
       "4965866  You'll need heavy duty shears to open packages...          1.0      1\n",
       "2225032  It didn't take the brassiness out of my hair a...          1.0      1\n",
       "3826255  I have a suspicion that this brush may be the ...          2.0      1\n",
       "4812345  I was so thrilled to find a cologne that has b...          1.0      1\n",
       "2084719  While the toothbrush does an excellent job cle...          1.0      1\n",
       "...                                                    ...          ...    ...\n",
       "3386104  The picture is deceptive in that you only get ...          4.0      3\n",
       "4127620  Most of these colors are very natural. A lot o...          4.0      3\n",
       "2404233                                      it does work.          4.0      3\n",
       "279257                             Exactly what I expected          5.0      3\n",
       "2552877                                 Love the fragrance          5.0      3\n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset[\"review_length\"] = [len(str(x)) for x in balanced_dataset[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average length of the reviews before cleaning.\n",
    "\n",
    "length_before_cleaning = balanced_dataset[\"review_length\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "balanced_dataset[\"review_body\"] = balanced_dataset[\"review_body\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "balanced_dataset[\"review_body\"] = [re.sub('<[^<]+?>', '', str(x)) for x in balanced_dataset[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "balanced_dataset[\"review_body\"] =  [re.sub(r\"http\\S+\",\"\", str(x)) for x in balanced_dataset[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "balanced_dataset[\"review_body\"] = [contractions.fix(str(x)) for x in balanced_dataset[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetical characters\n",
    "balanced_dataset[\"review_body\"] = [re.sub(r\"[^a-zA-Z ]\", \"\", str(x)) for x in balanced_dataset[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove excess spaces\n",
    "balanced_dataset[\"review_body\"] = balanced_dataset[\"review_body\"].replace(\"\\s+\", \" \", regex = True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset[\"review_length\"] = [len(str(x)) for x in balanced_dataset[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average length of the reviews after cleaning.\n",
    "\n",
    "length_after_cleaning = balanced_dataset[\"review_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269.7167333333333,258.84695\n"
     ]
    }
   ],
   "source": [
    "print(str(length_before_cleaning) + \",\" + str(length_after_cleaning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet = True)\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset[\"review_body_no_stopwords\"] = balanced_dataset[\"review_body\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert nltk pos tags into tags that WordNetLemmatizer can understand.\n",
    "\n",
    "def nltk_pos_converter(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return \"a\"\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return \"v\"\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another column for text without stopwords.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "balanced_dataset[\"review_body_no_stopwords\"] = balanced_dataset[\"review_body_no_stopwords\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, pos = nltk_pos_converter(tag)) for word, tag in nltk.pos_tag(x.split())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the text with stopwords.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "balanced_dataset[\"review_body\"] = balanced_dataset[\"review_body\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, pos = nltk_pos_converter(tag)) for word, tag in nltk.pos_tag(x.split())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "      <th>review_length</th>\n",
       "      <th>review_body_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4965866</th>\n",
       "      <td>you will need heavy duty shear to open package...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>516</td>\n",
       "      <td>need heavy duty shear open package like packag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225032</th>\n",
       "      <td>it do not take the brassiness out of my hair a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>445</td>\n",
       "      <td>take brassiness hair infact add brassiness can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826255</th>\n",
       "      <td>i have a suspicion that this brush may be the ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>436</td>\n",
       "      <td>suspicion brush may recent breakage experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4812345</th>\n",
       "      <td>i be so thrilled to find a cologne that have b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>472</td>\n",
       "      <td>thrill find cologne discountinued time even am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084719</th>\n",
       "      <td>while the toothbrush do an excellent job clean...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>toothbrush excellent job clean teeth manufactu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386104</th>\n",
       "      <td>the picture be deceptive in that you only get ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>566</td>\n",
       "      <td>picture deceptive get little guy arrive adorab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127620</th>\n",
       "      <td>most of these color be very natural a lot of b...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>183</td>\n",
       "      <td>color natural lot brown grey black white vibra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404233</th>\n",
       "      <td>it do work</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279257</th>\n",
       "      <td>exactly what i expect</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>exactly expect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552877</th>\n",
       "      <td>love the fragrance</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>love fragrance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating  \\\n",
       "4965866  you will need heavy duty shear to open package...          1.0   \n",
       "2225032  it do not take the brassiness out of my hair a...          1.0   \n",
       "3826255  i have a suspicion that this brush may be the ...          2.0   \n",
       "4812345  i be so thrilled to find a cologne that have b...          1.0   \n",
       "2084719  while the toothbrush do an excellent job clean...          1.0   \n",
       "...                                                    ...          ...   \n",
       "3386104  the picture be deceptive in that you only get ...          4.0   \n",
       "4127620  most of these color be very natural a lot of b...          4.0   \n",
       "2404233                                         it do work          4.0   \n",
       "279257                               exactly what i expect          5.0   \n",
       "2552877                                 love the fragrance          5.0   \n",
       "\n",
       "         class  review_length  \\\n",
       "4965866      1            516   \n",
       "2225032      1            445   \n",
       "3826255      1            436   \n",
       "4812345      1            472   \n",
       "2084719      1            253   \n",
       "...        ...            ...   \n",
       "3386104      3            566   \n",
       "4127620      3            183   \n",
       "2404233      3             12   \n",
       "279257       3             23   \n",
       "2552877      3             18   \n",
       "\n",
       "                                  review_body_no_stopwords  \n",
       "4965866  need heavy duty shear open package like packag...  \n",
       "2225032  take brassiness hair infact add brassiness can...  \n",
       "3826255  suspicion brush may recent breakage experience...  \n",
       "4812345  thrill find cologne discountinued time even am...  \n",
       "2084719  toothbrush excellent job clean teeth manufactu...  \n",
       "...                                                    ...  \n",
       "3386104  picture deceptive get little guy arrive adorab...  \n",
       "4127620  color natural lot brown grey black white vibra...  \n",
       "2404233                                               work  \n",
       "279257                                      exactly expect  \n",
       "2552877                                     love fragrance  \n",
       "\n",
       "[60000 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset[\"review_length\"] = [len(str(x)) for x in balanced_dataset[\"review_body_no_stopwords\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average length of reviews after preprocessing (Stopwords removed).\n",
    "\n",
    "length_after_preprocessing = balanced_dataset[\"review_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258.84695,151.47631666666666\n"
     ]
    }
   ],
   "source": [
    "print(str(length_after_cleaning) + \",\" + str(length_after_preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset[\"review_length\"] = [len(str(x)) for x in balanced_dataset[\"review_body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average length of reviews after preprocessing (Stopwords retained).\n",
    "\n",
    "length_after_preprocessing = balanced_dataset[\"review_length\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258.84695,248.88255\n"
     ]
    }
   ],
   "source": [
    "print(str(length_after_cleaning) + \",\" + str(length_after_preprocessing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.0001, max_df = 0.5, ngram_range = (1,3))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(balanced_dataset[\"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_features\n",
    "y = balanced_dataset[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets with stopwords.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.0001, max_df = 0.5, ngram_range = (1,3))\n",
    "tfidf_features_without_stopwords = tfidf_vectorizer.fit_transform(balanced_dataset[\"review_body_no_stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_features_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets without stopwords.\n",
    "\n",
    "X_train_wos, X_test_wos, y_train_wos, y_test_wos = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the metrics and print them.\n",
    "\n",
    "def metrics_calculator(y_test, y_pred):\n",
    "    class_1_cm = dict()\n",
    "    class_2_cm = dict()\n",
    "    class_3_cm = dict()\n",
    "    \n",
    "    confusion_matrix = [[0]*3 for i in range(3)]\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] == 1:\n",
    "            if y_pred[i] == 1:\n",
    "                confusion_matrix[0][0] += 1\n",
    "            elif y_pred[i] == 2:\n",
    "                confusion_matrix[0][1] += 1\n",
    "            else:\n",
    "                confusion_matrix[0][2] += 1\n",
    "        elif y_test[i] == 2:\n",
    "            if y_pred[i] == 1:\n",
    "                confusion_matrix[1][0] += 1\n",
    "            elif y_pred[i] == 2:\n",
    "                confusion_matrix[1][1] += 1\n",
    "            else:\n",
    "                confusion_matrix[1][2] += 1\n",
    "        else:\n",
    "            if y_pred[i] == 1:\n",
    "                confusion_matrix[2][0] += 1\n",
    "            elif y_pred[i] == 2:\n",
    "                confusion_matrix[2][1] += 1\n",
    "            else:\n",
    "                confusion_matrix[2][2] += 1\n",
    "        \n",
    "    class_1_tp = confusion_matrix[0][0]\n",
    "    class_1_tn = confusion_matrix[1][1] + confusion_matrix[1][2] + confusion_matrix[2][1] + confusion_matrix[2][2]\n",
    "    class_1_fp = confusion_matrix[1][0] + confusion_matrix[2][0]\n",
    "    class_1_fn = confusion_matrix[0][1] + confusion_matrix[0][2]\n",
    "    \n",
    "    class_2_tp = confusion_matrix[1][1]\n",
    "    class_2_tn = confusion_matrix[0][0] + confusion_matrix[0][2] + confusion_matrix[2][0] + confusion_matrix[2][2]\n",
    "    class_2_fp = confusion_matrix[0][1] + confusion_matrix[2][1]\n",
    "    class_2_fn = confusion_matrix[1][0] + confusion_matrix[1][2]\n",
    "    \n",
    "    class_3_tp = confusion_matrix[2][2]\n",
    "    class_3_tn = confusion_matrix[0][0] + confusion_matrix[0][1] + confusion_matrix[1][0] + confusion_matrix[1][1]\n",
    "    class_3_fp = confusion_matrix[0][2] + confusion_matrix[1][2]\n",
    "    class_3_fn = confusion_matrix[2][0] + confusion_matrix[2][1]\n",
    "    \n",
    "    class_1_precision = (class_1_tp) / (class_1_tp + class_1_fp)\n",
    "    class_1_recall = (class_1_tp) / (class_1_tp + class_1_fn)\n",
    "    class_1_f1_score = 2 * class_1_precision * class_1_recall / (class_1_precision + class_1_recall)\n",
    "    \n",
    "    class_2_precision = (class_2_tp) / (class_2_tp + class_2_fp)\n",
    "    class_2_recall = (class_2_tp) / (class_2_tp + class_2_fn)\n",
    "    class_2_f1_score = 2 * class_2_precision * class_2_recall / (class_2_precision + class_2_recall)\n",
    "    \n",
    "    class_3_precision = (class_3_tp) / (class_3_tp + class_3_fp)\n",
    "    class_3_recall = (class_3_tp) / (class_3_tp + class_3_fn)\n",
    "    class_3_f1_score = 2 * class_3_precision * class_3_recall / (class_3_precision + class_3_recall)\n",
    "    \n",
    "    print(str(class_1_precision) + \",\" + str(class_1_recall) + \",\" + str(class_1_f1_score))  \n",
    "    print(str(class_2_precision) + \",\" + str(class_2_recall) + \",\" + str(class_2_f1_score))  \n",
    "    print(str(class_3_precision) + \",\" + str(class_3_recall) + \",\" + str(class_3_f1_score))  \n",
    "    \n",
    "    average_precision = (class_1_precision + class_2_precision + class_3_precision)/3\n",
    "    average_recall = (class_1_recall + class_2_recall + class_3_recall)/3\n",
    "    average_f1_score = (class_1_f1_score + class_2_f1_score + class_3_f1_score)/3\n",
    "    \n",
    "    print(str(average_precision) + \",\" + str(average_recall) + \",\" + str(average_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5979827089337176,0.6264150943396226,0.6118687799483966\n",
      "0.5076035658101731,0.49137055837563454,0.4993551715243745\n",
      "0.6954251616111388,0.6847001223990208,0.6900209695325028\n",
      "0.6003371454516765,0.6008285917047593,0.6004149736684247\n"
     ]
    }
   ],
   "source": [
    "# Perceptron without stopwords.\n",
    "\n",
    "perceptron = Perceptron(alpha = 0.0001, tol = 1e-3)\n",
    "perceptron.fit(X_train_wos, y_train_wos)\n",
    "y_pred_wos = perceptron.predict(X_test_wos)\n",
    "metrics_calculator(y_test_wos.values, y_pred_wos)\n",
    "\n",
    "# 0.5979827089337176,0.6264150943396226,0.6118687799483966\n",
    "# 0.5076035658101731,0.49137055837563454,0.4993551715243745\n",
    "# 0.6954251616111388,0.6847001223990208,0.6900209695325028\n",
    "# 0.6003371454516765,0.6008285917047593,0.6004149736684247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6828846628797234,0.688667496886675,0.685763888888889\n",
      "0.5745800952619704,0.5727136431784108,0.5736453510198974\n",
      "0.773346794548208,0.7692693949284459,0.7713027061044683\n",
      "0.6769371842299673,0.6768835116645106,0.6769039820044181\n"
     ]
    }
   ],
   "source": [
    "# Perceptron with stopwords.\n",
    "\n",
    "perceptron = Perceptron(alpha = 0.0001, tol = 1e-3)\n",
    "perceptron.fit(X_train, y_train)\n",
    "y_pred = perceptron.predict(X_test)\n",
    "metrics_calculator(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662357036300348,0.670188679245283,0.6662498436913843\n",
      "0.577438370846731,0.5469543147208121,0.5617831074035454\n",
      "0.7416391898257183,0.7708690330477356,0.7559716720681791\n",
      "0.6604781989909325,0.6626706756712769,0.6613348743877029\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC without scaling and without stopwords.\n",
    "\n",
    "lsvc = LinearSVC(C = 1.0, tol = 1e-3)\n",
    "lsvc.fit(X_train_wos, y_train_wos)\n",
    "y_pred_wos = lsvc.predict(X_test_wos)\n",
    "metrics_calculator(y_test_wos.values, y_pred_wos)\n",
    "\n",
    "# 0.662357036300348,0.670188679245283,0.6662498436913843\n",
    "# 0.577438370846731,0.5469543147208121,0.5617831074035454\n",
    "# 0.7416391898257183,0.7708690330477356,0.7559716720681791\n",
    "# 0.6604781989909325,0.6626706756712769,0.6613348743877029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662357036300348,0.670188679245283,0.6662498436913843\n",
      "0.577438370846731,0.5469543147208121,0.5617831074035454\n",
      "0.7416391898257183,0.7708690330477356,0.7559716720681791\n",
      "0.6604781989909325,0.6626706756712769,0.6613348743877029\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with scaling and without stopwords.\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(with_mean = False), LinearSVC(C = 1.0, tol = 1e-3))\n",
    "pipeline.fit(X_train_wos, y_train_wos)\n",
    "y_pred = pipeline.predict(X_test_wos)\n",
    "metrics_calculator(y_test_wos.values, y_pred_wos)\n",
    "\n",
    "# 0.662357036300348,0.670188679245283,0.6662498436913843\n",
    "# 0.577438370846731,0.5469543147208121,0.5617831074035454\n",
    "# 0.7416391898257183,0.7708690330477356,0.7559716720681791\n",
    "# 0.6604781989909325,0.6626706756712769,0.6613348743877029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7243842364532019,0.7325031133250312,0.7284210526315789\n",
      "0.6320467242254951,0.6219390304847576,0.6269521410579346\n",
      "0.811344327836082,0.8152146623148381,0.8132748904195367\n",
      "0.7225917628382597,0.7232189353748756,0.7228826947030167\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC without scaling and with stopwords.\n",
    "\n",
    "lsvc = LinearSVC(C = 1.0, tol = 1e-5)\n",
    "lsvc.fit(X_train, y_train)\n",
    "y_pred = lsvc.predict(X_test)\n",
    "metrics_calculator(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6495160468670402,0.635118306351183,0.6422364941443143\n",
      "0.5422086202499362,0.5312343828085957,0.5366654045184905\n",
      "0.713700939080183,0.7441626914386141,0.7286135693215339\n",
      "0.6351418687323865,0.6368384601994643,0.6358384893281129\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC with scaling and with stopwords.\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(with_mean = False), LinearSVC(C = 1.0, tol = 1e-3))\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "metrics_calculator(y_test.values, y_pred)\n",
    "\n",
    "# 0.6495160468670402,0.635118306351183,0.6422364941443143\n",
    "# 0.5422086202499362,0.5312343828085957,0.5366654045184905\n",
    "# 0.713700939080183,0.7441626914386141,0.7286135693215339\n",
    "# 0.6351418687323865,0.6368384601994643,0.6358384893281129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6908588648920141,0.6920754716981132,0.6914666331531985\n",
      "0.5923927011051143,0.5850253807106599,0.5886859915719576\n",
      "0.7685970438575236,0.7764993880048959,0.7725280077934729\n",
      "0.6839495366182172,0.6845334134712231,0.6842268775062097\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression without stopwords.\n",
    "\n",
    "logreg = LogisticRegression(C = 1.0, tol = 1e-3, max_iter = 1000)\n",
    "logreg.fit(X_train_wos, y_train_wos)\n",
    "y_pred_wos = logreg.predict(X_test_wos)\n",
    "metrics_calculator(y_test_wos.values, y_pred_wos)\n",
    "\n",
    "# 0.6908588648920141,0.6920754716981132,0.6914666331531985\n",
    "# 0.5923927011051143,0.5850253807106599,0.5886859915719576\n",
    "# 0.7685970438575236,0.7764993880048959,0.7725280077934729\n",
    "# 0.6839495366182172,0.6845334134712231,0.6842268775062097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7475,0.7447073474470735,0.746101060511541\n",
      "0.6444007858546169,0.655672163918041,0.6499876145652712\n",
      "0.8294297352342159,0.8179763996987196,0.8236632536973835\n",
      "0.740443507029611,0.7394519703546113,0.7399173095913986\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with stopwords.\n",
    "\n",
    "logreg = LogisticRegression(C = 1.0, tol = 1e-3, max_iter = 1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "metrics_calculator(y_test.values, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6936582809224319,0.6659119496855346,0.6795019894750353\n",
      "0.5726536445926632,0.6101522842639594,0.5908085524698944\n",
      "0.7601605619668841,0.7417380660954712,0.7508363275926156\n",
      "0.6754908291606597,0.672600766681655,0.6737156231791818\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes without stopwords.\n",
    "\n",
    "mnb = MultinomialNB(alpha = 1.0)\n",
    "mnb.fit(X_train_wos, y_train_wos)\n",
    "y_pred_wos = mnb.predict(X_test_wos)\n",
    "metrics_calculator(y_test_wos.values, y_pred_wos)\n",
    "\n",
    "# 0.6936582809224319,0.6659119496855346,0.6795019894750353\n",
    "# 0.5726536445926632,0.6101522842639594,0.5908085524698944\n",
    "# 0.7601605619668841,0.7417380660954712,0.7508363275926156\n",
    "# 0.6754908291606597,0.672600766681655,0.6737156231791818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7457237681899412,0.7275217932752179,0.7365103378719111\n",
      "0.627000695894224,0.6754122938530734,0.6503067484662577\n",
      "0.8374867444326617,0.7931207632437861,0.8147001934235976\n",
      "0.736737069505609,0.7320182834573591,0.7338390932539222\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with stopwords.\n",
    "\n",
    "mnb = MultinomialNB(alpha = 1.0)\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "metrics_calculator(y_test.values, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
